{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74fa173-6ff2-467e-af12-622a30b2231a",
   "metadata": {},
   "source": [
    "# Vector Database Collection & API Setup\n",
    "\n",
    "This notebook will walk you through setting up the vector database portion of the [openai-realtime-rag](https://github.com/ALucek/openai-realtime-rag/tree/main) fork."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b5b9d-5aa2-4a07-8d2b-2de0c9c6bfb4",
   "metadata": {},
   "source": [
    "## Setting Up Your Vector Database\n",
    "\n",
    "For our vector database, a classic choice I use is [ChromaDB](https://www.trychroma.com/). While you can host Chroma as a server itself, I've decoupled the database and the API to allow for more dynamic plug and play capabilities for databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa07129-78ad-4e64-8818-c2d51e3d6bda",
   "metadata": {},
   "source": [
    "#### Instantiate ChromaDB\n",
    "\n",
    "Create a persistent client of ChromaDB that will store everything in the folder `chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb PyMuPDF tiktoken langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2af23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!.\\venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb PyMuPDF langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3c036b-0cff-4fa6-b42e-5619429df7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Creating Vector Database\n",
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd5c72-3905-4062-b561-2238b16512c1",
   "metadata": {},
   "source": [
    "#### Create a New Collection\n",
    "\n",
    "This is where all of our chunked text documents are going to be inserted into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d874dd-23e7-4766-99a9-899bca877a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(name=\"vdb_collection\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae864-a47a-4b01-a785-d015cb053b0a",
   "metadata": {},
   "source": [
    "#### Load & Split PDF \n",
    "\n",
    "We'll be using some simple LangChain integrations to load and chunk our PDF. Using OpenAI's standard token chunk size and overlap for their Assistants API as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430078d0-7ec1-4a78-9f9b-569e4a3b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Define a directory where your PDFs are stored\n",
    "pdf_directory = \"./pdfs/\"  # You'll need to create this directory and put your PDFs there\n",
    "\n",
    "# Function to process multiple PDFs\n",
    "def process_pdfs(directory):\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Iterate through all PDF files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Load and process each PDF\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            \n",
    "            document = \"\"\n",
    "            for page in pages:\n",
    "                document += page.page_content\n",
    "            \n",
    "            # Split into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                model_name=\"gpt-4\",\n",
    "                chunk_size=800,\n",
    "                chunk_overlap=400,\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_text(document)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a17d79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Artificial Intelligence A New Dawn.pdf\n",
      "Processing: eMarketing.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:300: UserWarning: Warning: Empty content on page 0 of document ./pdfs/eMarketing.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1040\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs\n",
    "chunks = process_pdfs(pdf_directory)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73682f57-99fa-4e1d-8d87-96ac2aef779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47642d-ede7-42bb-9323-e9170ebe01c7",
   "metadata": {},
   "source": [
    "#### Insert Chunks into VDB Collection\n",
    "\n",
    "Embed each chunk into the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce459ee9-69a9-4387-bb1b-d101916390f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Chunks into ChromaDB Collection\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    collection.add(\n",
    "    documents=[chunk],\n",
    "    ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605248d0-09f6-44d6-89db-e01af12d60f4",
   "metadata": {},
   "source": [
    "#### If Repopulating the DB, first delete the collection\n",
    "\n",
    "Use this if you need to reset your vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874ef72-623a-4177-b130-5a3aadb1b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"vdb_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022acb9c-4f13-47a5-9453-003101da178a",
   "metadata": {},
   "source": [
    "---\n",
    "## API Setup\n",
    "\n",
    "We'll be using [FastAPI](https://fastapi.tiangolo.com/) as a quick and easy way to host our query function as a REST API. This API is what will be called from the defined `query_db` tool in the main console file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f958037-4293-4f09-9231-24da27a49956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define a request model\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Define the query endpoint\n",
    "@app.post(\"/query\")\n",
    "async def query_chroma(request: QueryRequest):\n",
    "    # Perform the query on your ChromaDB collection\n",
    "    results = collection.query(query_texts=[request.query], n_results=5)\n",
    "    return {\"results\": results['documents'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b3cfb-8067-400a-8f77-eecceb93122c",
   "metadata": {},
   "source": [
    "#### Run API\n",
    "\n",
    "Using uvicorn to host the API as a local web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "582c4a6f-77bc-447a-9e56-95c84033dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [10076]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55485 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55485 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55613 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "def run_api():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Run the FastAPI app in a background thread\n",
    "thread = threading.Thread(target=run_api)\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
