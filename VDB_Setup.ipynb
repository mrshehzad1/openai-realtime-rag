{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74fa173-6ff2-467e-af12-622a30b2231a",
   "metadata": {},
   "source": [
    "# Vector Database Collection & API Setup\n",
    "\n",
    "This notebook will walk you through setting up the vector database portion of the [openai-realtime-rag](https://github.com/ALucek/openai-realtime-rag/tree/main) fork."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b5b9d-5aa2-4a07-8d2b-2de0c9c6bfb4",
   "metadata": {},
   "source": [
    "## Setting Up Your Vector Database\n",
    "\n",
    "For our vector database, a classic choice I use is [ChromaDB](https://www.trychroma.com/). While you can host Chroma as a server itself, I've decoupled the database and the API to allow for more dynamic plug and play capabilities for databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa07129-78ad-4e64-8818-c2d51e3d6bda",
   "metadata": {},
   "source": [
    "#### Instantiate ChromaDB\n",
    "\n",
    "Create a persistent client of ChromaDB that will store everything in the folder `chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3c036b-0cff-4fa6-b42e-5619429df7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Creating Vector Database\n",
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd5c72-3905-4062-b561-2238b16512c1",
   "metadata": {},
   "source": [
    "#### Create a New Collection\n",
    "\n",
    "This is where all of our chunked text documents are going to be inserted into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d874dd-23e7-4766-99a9-899bca877a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(name=\"vdb_collection\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae864-a47a-4b01-a785-d015cb053b0a",
   "metadata": {},
   "source": [
    "#### Load & Split PDF \n",
    "\n",
    "We'll be using some simple LangChain integrations to load and chunk our PDF. Using OpenAI's standard token chunk size and overlap for their Assistants API as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430078d0-7ec1-4a78-9f9b-569e4a3b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loading and Chunking\n",
    "loader = PyMuPDFLoader(\"./eMarketing.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "document = \"\"\n",
    "for i in range(len(pages)):\n",
    "    document += pages[i].page_content\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73682f57-99fa-4e1d-8d87-96ac2aef779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47642d-ede7-42bb-9323-e9170ebe01c7",
   "metadata": {},
   "source": [
    "#### Insert Chunks into VDB Collection\n",
    "\n",
    "Embed each chunk into the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce459ee9-69a9-4387-bb1b-d101916390f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Chunks into ChromaDB Collection\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    collection.add(\n",
    "    documents=[chunk],\n",
    "    ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605248d0-09f6-44d6-89db-e01af12d60f4",
   "metadata": {},
   "source": [
    "#### If Repopulating the DB, first delete the collection\n",
    "\n",
    "Use this if you need to reset your vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874ef72-623a-4177-b130-5a3aadb1b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"vdb_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022acb9c-4f13-47a5-9453-003101da178a",
   "metadata": {},
   "source": [
    "---\n",
    "## API Setup\n",
    "\n",
    "We'll be using [FastAPI](https://fastapi.tiangolo.com/) as a quick and easy way to host our query function as a REST API. This API is what will be called from the defined `query_db` tool in the main console file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f958037-4293-4f09-9231-24da27a49956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define a request model\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Define the query endpoint\n",
    "@app.post(\"/query\")\n",
    "async def query_chroma(request: QueryRequest):\n",
    "    # Perform the query on your ChromaDB collection\n",
    "    results = collection.query(query_texts=[request.query], n_results=5)\n",
    "    return {\"results\": results['documents'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b3cfb-8067-400a-8f77-eecceb93122c",
   "metadata": {},
   "source": [
    "#### Run API\n",
    "\n",
    "Using uvicorn to host the API as a local web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582c4a6f-77bc-447a-9e56-95c84033dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [74278]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:62604 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62778 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62895 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63056 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65533 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65533 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:49185 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "def run_api():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Run the FastAPI app in a background thread\n",
    "thread = threading.Thread(target=run_api)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471babe-b2f8-4e3d-aeac-03e076a6e112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
